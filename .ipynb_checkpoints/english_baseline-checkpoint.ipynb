{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51c4db13-918e-46cf-a8d5-89285ad62b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import RobertaForSequenceClassification, RobertaTokenizer, Trainer, TrainingArguments\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c35f562f-c314-4a15-b279-f5b0b3ab3725",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"data/subtask-2-english/train_en.tsv\",sep='\\t')\n",
    "dev_data = pd.read_csv(\"data/subtask-2-english/dev_en.tsv\", sep='\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "453ce330-db05-4ad1-b504-01003af28b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      0\n",
      "1      0\n",
      "2      0\n",
      "3      0\n",
      "4      0\n",
      "      ..\n",
      "795    0\n",
      "796    0\n",
      "797    0\n",
      "798    0\n",
      "799    0\n",
      "Name: label, Length: 800, dtype: int64\n",
      "0      0\n",
      "1      1\n",
      "2      0\n",
      "3      0\n",
      "4      0\n",
      "      ..\n",
      "195    0\n",
      "196    1\n",
      "197    1\n",
      "198    1\n",
      "199    0\n",
      "Name: label, Length: 200, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Mapping label strings to integers\n",
    "label_map = {\"OBJ\": 0, \"SUBJ\": 1}\n",
    "train_data['label'] = train_data['label'].map(label_map)\n",
    "print(train_data['label'])\n",
    "\n",
    "dev_data['label'] = dev_data['label'].map(label_map)\n",
    "print(dev_data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "82bab64a-e3e9-4737-88f8-3069d51eea71",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['solved_conflict'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_data \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msolved_conflict\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\pandas\\core\\frame.py:5568\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   5420\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdrop\u001b[39m(\n\u001b[0;32m   5421\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   5422\u001b[0m     labels: IndexLabel \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5429\u001b[0m     errors: IgnoreRaise \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   5430\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   5431\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   5432\u001b[0m \u001b[38;5;124;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[0;32m   5433\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5566\u001b[0m \u001b[38;5;124;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[0;32m   5567\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 5568\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   5569\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5570\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5571\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5572\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5573\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5574\u001b[0m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5575\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5576\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\pandas\\core\\generic.py:4782\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4780\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m   4781\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 4782\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_drop_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4784\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[0;32m   4785\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\pandas\\core\\generic.py:4824\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[1;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[0;32m   4822\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m   4823\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 4824\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m \u001b[43maxis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4825\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mget_indexer(new_axis)\n\u001b[0;32m   4827\u001b[0m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[0;32m   4828\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:7069\u001b[0m, in \u001b[0;36mIndex.drop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   7067\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m   7068\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 7069\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabels[mask]\u001b[38;5;241m.\u001b[39mtolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in axis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   7070\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[\u001b[38;5;241m~\u001b[39mmask]\n\u001b[0;32m   7071\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelete(indexer)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['solved_conflict'] not found in axis\""
     ]
    }
   ],
   "source": [
    "train_data = train_data.drop('solved_conflict', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b0e43fd-b039-4889-a78e-95c25006ceb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4f9c8bcd60318b0d1257f35ebc7c4ede9f7930e1</td>\n",
       "      <td>Die Ausbreitung des Virus sei beschränkter als...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0531b165e42997e8eecbb84d1e774c728041db8c</td>\n",
       "      <td>Zwar sei ein Anstieg der Zahlen zu verzeichnen.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c34bac7a38d7959b0d3c340810f5d2ac3187a3ec</td>\n",
       "      <td>Der Arbeitsmarkt hat sich so zugunsten der Lei...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3c6b7daf4e0cce25f45c8497da669c8936a66113</td>\n",
       "      <td>Es wird das PCR-Pooling mit Antigen-Schnelltes...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9d241c839c8b801bdbc526cd6e20f70ecd11f341</td>\n",
       "      <td>Als er bemerkte, dass Internetbetreiber die Ge...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>795</th>\n",
       "      <td>26742376756a241814a3e089349d1c08dd267012</td>\n",
       "      <td>Der lettische Regierungschef Krisjans Karins h...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>796</th>\n",
       "      <td>6c68576e3227cd0ae611abe6527e441c196a4e6d</td>\n",
       "      <td>Das Angiotensin-konvertierende Enzym 2 wird ha...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>797</th>\n",
       "      <td>5c2da5d7dc995638e388fa7c0a8b643bdd266a18</td>\n",
       "      <td>„In enger Abstimmung mit dem Ministerium für A...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>798</th>\n",
       "      <td>7f2b3c069add8f1db904077337748b6c79d659be</td>\n",
       "      <td>Unter anderem sollen die Patrouillen an Land u...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799</th>\n",
       "      <td>c689bd94d876c250724f1e1c6bec84aabff923ea</td>\n",
       "      <td>Am Mittwoch der vergangenen Woche waren zunäch...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>800 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  sentence_id  \\\n",
       "0    4f9c8bcd60318b0d1257f35ebc7c4ede9f7930e1   \n",
       "1    0531b165e42997e8eecbb84d1e774c728041db8c   \n",
       "2    c34bac7a38d7959b0d3c340810f5d2ac3187a3ec   \n",
       "3    3c6b7daf4e0cce25f45c8497da669c8936a66113   \n",
       "4    9d241c839c8b801bdbc526cd6e20f70ecd11f341   \n",
       "..                                        ...   \n",
       "795  26742376756a241814a3e089349d1c08dd267012   \n",
       "796  6c68576e3227cd0ae611abe6527e441c196a4e6d   \n",
       "797  5c2da5d7dc995638e388fa7c0a8b643bdd266a18   \n",
       "798  7f2b3c069add8f1db904077337748b6c79d659be   \n",
       "799  c689bd94d876c250724f1e1c6bec84aabff923ea   \n",
       "\n",
       "                                              sentence  label  \n",
       "0    Die Ausbreitung des Virus sei beschränkter als...      0  \n",
       "1      Zwar sei ein Anstieg der Zahlen zu verzeichnen.      0  \n",
       "2    Der Arbeitsmarkt hat sich so zugunsten der Lei...      0  \n",
       "3    Es wird das PCR-Pooling mit Antigen-Schnelltes...      0  \n",
       "4    Als er bemerkte, dass Internetbetreiber die Ge...      0  \n",
       "..                                                 ...    ...  \n",
       "795  Der lettische Regierungschef Krisjans Karins h...      0  \n",
       "796  Das Angiotensin-konvertierende Enzym 2 wird ha...      0  \n",
       "797  „In enger Abstimmung mit dem Ministerium für A...      0  \n",
       "798  Unter anderem sollen die Patrouillen an Land u...      0  \n",
       "799  Am Mittwoch der vergangenen Woche waren zunäch...      0  \n",
       "\n",
       "[800 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb5e64bf-6878-4798-909e-8cafd20d509d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "from transformers import XLMRobertaTokenizer, XLMRobertaForSequenceClassification, Trainer, TrainingArguments\n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9316a84c-fbfa-4470-a6d6-7bd982b1d878",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings = tokenizer(train_data['sentence'].tolist(), truncation=True, padding=True)\n",
    "eval_encodings = tokenizer(dev_data['sentence'].tolist(), truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8410770-d13a-4171-a1ab-8649cea1a448",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_encodings[\"input_ids\"]\n",
    "#train_encodings[\"attention_mask\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54d733c6-fe92-4669-be32-2d41330930c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PyTorch datasets\n",
    "train_dataset = torch.utils.data.TensorDataset(\n",
    "    torch.tensor(train_encodings['input_ids']),\n",
    "    torch.tensor(train_encodings['attention_mask']),\n",
    "    torch.tensor(train_data['label'])\n",
    ")\n",
    "\n",
    "val_dataset = torch.utils.data.TensorDataset(\n",
    "    torch.tensor(eval_encodings['input_ids']),\n",
    "    torch.tensor(eval_encodings['attention_mask']),\n",
    "    torch.tensor(dev_data['label'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fef0ac7b-eddf-4d72-a6e5-ed40fd5c0704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([157])\n",
      "torch.Size([157])\n",
      "torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[0][0].shape) # input_ids shape\n",
    "print(train_dataset[0][1].shape) # attention_mask shape\n",
    "print(train_dataset[0][2].shape) # train_labels_onehot shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e52d0654-f37c-45a9-8cf0-f0eb9394f709",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Define model\n",
    "model = XLMRobertaForSequenceClassification.from_pretrained('xlm-roberta-large', num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5047e6e-4428-4172-bcf1-582d78d540c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer parameters\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "561d3b8a-6e2e-4e74-8d5b-365031776a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=5,  # Train for more epochs\n",
    "    per_device_train_batch_size=8,  # Decrease batch size for better generalization\n",
    "    per_device_eval_batch_size=8,  # Keep evaluation batch size consistent with training\n",
    "    warmup_steps=1000,  # Increase warmup steps for longer training\n",
    "    weight_decay=0.001,  # Slightly decrease weight decay for regularization\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=50,  # Log less frequently for faster training\n",
    "    evaluation_strategy='steps',\n",
    "    eval_steps=100,  # Evaluate less frequently for faster training\n",
    "    learning_rate=2e-5,  # Slightly decrease learning rate for smoother training\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "493e86e5-a63f-40dc-b6bb-09e3bf48a3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=lambda data: {'input_ids': torch.stack([item[0] for item in data]),\n",
    "                                'attention_mask': torch.stack([item[1] for item in data]),\n",
    "                                'labels': torch.stack([item[2] for item in data])},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "96eb11d6-0c5c-4d85-b10d-0e2a028b0f8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ca03ece810b43cc8042d90c0e949877",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6898, 'grad_norm': 8.063957214355469, 'learning_rate': 1e-05, 'epoch': 2.0}\n",
      "{'train_runtime': 4488.6922, 'train_samples_per_second': 0.535, 'train_steps_per_second': 0.033, 'train_loss': 0.6680028279622395, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=150, training_loss=0.6680028279622395, metrics={'train_runtime': 4488.6922, 'train_samples_per_second': 0.535, 'train_steps_per_second': 0.033, 'total_flos': 685843237699200.0, 'train_loss': 0.6680028279622395, 'epoch': 3.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# epoch 4 / 50 = 200\t0.538800\t0.644553"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d7a43a6c-c16c-4727-a3e0-535b08399f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"./model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "be77525e-b65e-4001-9cf2-e944bf62f142",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = XLMRobertaForSequenceClassification.from_pretrained(\"./model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0ed6e01f-b6c1-4fbc-b6eb-08fb20e1984f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "test_data = pd.read_csv(\"data/subtask-2-english/dev_test_en.tsv\", sep='\\t')  # Update with your dev data file\n",
    "test_data['label'] = test_data['label'].map(label_map)\n",
    "test_encodings = tokenizer(test_data['sentence'].tolist(), truncation=True, padding=True)\n",
    "\n",
    "test_dataset = torch.utils.data.TensorDataset(\n",
    "    torch.tensor(test_encodings['input_ids']),\n",
    "    torch.tensor(test_encodings['attention_mask']),\n",
    "    torch.tensor(test_data['label'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e27e6a25-370d-46ca-a31a-7ecaa349c6fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "542b692b314b4b6d83ac1b1ad4e15baa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preds = trainer.predict(test_dataset)\n",
    "pred_labels = preds.predictions.argmax(-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "43978ae1-955e-4775-8566-3b654114c453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 0 1 0 0 0 1 1 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1\n",
      " 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 1 0 0 1 0 1 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 1 0 1 1 0 0 0 1 1 1 1 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 1 0 1 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "150f12f8-f0cd-4a7d-aa88-d18edb5c33f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for RoBERTa: 0.5967078189300411\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         OBJ       0.55      0.89      0.68       116\n",
      "        SUBJ       0.76      0.33      0.46       127\n",
      "\n",
      "    accuracy                           0.60       243\n",
      "   macro avg       0.66      0.61      0.57       243\n",
      "weighted avg       0.66      0.60      0.56       243\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPTNeoForSequenceClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-2.7B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for ChatGPT-4: 0.4773662551440329\n",
      "Classification Report for ChatGPT-4:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         OBJ       0.48      1.00      0.65       116\n",
      "        SUBJ       1.00      0.00      0.00       127\n",
      "\n",
      "    accuracy                           0.48       243\n",
      "   macro avg       0.74      0.50      0.32       243\n",
      "weighted avg       0.75      0.48      0.31       243\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert the true labels to integers\n",
    "\n",
    "# Compute the accuracy and classification report\n",
    "accuracy = accuracy_score(test_data['label'], pred_labels)\n",
    "class_report = classification_report(test_data['label'], pred_labels, target_names=['OBJ', 'SUBJ'])\n",
    "\n",
    "print(f\"Accuracy for RoBERTa: {accuracy}\")\n",
    "print(f\"Classification Report:\\n{class_report}\")\n",
    "\n",
    "\n",
    "# Now, let's use ChatGPT-4 for evaluation\n",
    "from transformers import pipeline\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import pandas as pd\n",
    "\n",
    "# Load the ChatGPT-4 text classification pipeline\n",
    "chatgpt4_classifier = pipeline(\"text-classification\", model=\"EleutherAI/gpt-neo-2.7B\")\n",
    "\n",
    "# Load the test data\n",
    "test_data = pd.read_csv(\"data/subtask-2-english/dev_test_en.tsv\", sep='\\t')\n",
    "\n",
    "# Mapping label strings to integers\n",
    "label_map = {\"OBJ\": 0, \"SUBJ\": 1}\n",
    "test_data['label'] = test_data['label'].map(label_map)\n",
    "\n",
    "# Make predictions using ChatGPT-4\n",
    "chatgpt4_predictions = chatgpt4_classifier(test_data['sentence'].tolist())\n",
    "\n",
    "# Extract predicted labels from ChatGPT-4 predictions\n",
    "chatgpt4_pred_labels = [1 if pred[\"label\"] == \"SUBJ\" else 0 for pred in chatgpt4_predictions]\n",
    "\n",
    "# Compute accuracy and classification report for ChatGPT-4\n",
    "chatgpt4_accuracy = accuracy_score(test_data['label'], chatgpt4_pred_labels)\n",
    "chatgpt4_class_report = classification_report(test_data['label'], chatgpt4_pred_labels, target_names=['OBJ', 'SUBJ'], zero_division=1)\n",
    "\n",
    "print(f\"Accuracy for ChatGPT-4: {chatgpt4_accuracy}\")\n",
    "print(f\"Classification Report for ChatGPT-4:\\n{chatgpt4_class_report}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "67dacc59-1c15-4158-9d99-edd69d90bd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs 3 eval steps 50 - acc 0.58 - OBJ 0.54, SUBJ 0.81\n",
    "# epochs 10 eval steps 60 - BROKE\n",
    "# epochs 2 eval steps 50 - acc 0.61 - OBJ 0.56, SUBJ 0.71 - f1 0.66 - 0.54\n",
    "# epochs 4 eval steps 50 - acc 0.70 - OBJ 0.67, SUBJ 0.74 - f1 0.70 - 0.70\n",
    "# epochs 5 eval steps 50 - acc 0.79 - OBJ 0.85, SUBJ 0.75 - f1 0.76 - 0.82"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
