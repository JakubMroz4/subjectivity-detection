{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51c4db13-918e-46cf-a8d5-89285ad62b38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kuba\\Documents\\py_interpreter2\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Import STOPWORDS from NLTK\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import string, re\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import RobertaForSequenceClassification, RobertaTokenizer, Trainer, TrainingArguments\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c35f562f-c314-4a15-b279-f5b0b3ab3725",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_raw = pd.read_csv(\"data/subtask-2-multilingual/ml_only_2024_languages/train_ml_only_2024_languages.tsv\",sep='\\t')\n",
    "#dev_data = pd.read_csv(\"data/subtask-2-multilingual/ml_only_2024_languages/dev_ml_only_2024_languages.tsv\", sep='\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d809d3-cdd2-4725-9c3d-b9ebe60b196d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c53ea713-6960-4720-8fb9-c0190c29d2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data_raw.sample(n=1500)\n",
    "dev_data = train_data_raw.sample(n=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "453ce330-db05-4ad1-b504-01003af28b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3973    0\n",
      "2457    0\n",
      "1422    1\n",
      "1601    0\n",
      "1817    1\n",
      "       ..\n",
      "3766    1\n",
      "2828    0\n",
      "2638    1\n",
      "3974    1\n",
      "237     0\n",
      "Name: label, Length: 1500, dtype: int64\n",
      "910     0\n",
      "1591    0\n",
      "3211    0\n",
      "756     1\n",
      "1205    0\n",
      "       ..\n",
      "1258    0\n",
      "2309    1\n",
      "2188    0\n",
      "2143    0\n",
      "1317    0\n",
      "Name: label, Length: 300, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Mapping label strings to integers\n",
    "label_map = {\"OBJ\": 0, \"SUBJ\": 1}\n",
    "train_data['label'] = train_data['label'].map(label_map)\n",
    "print(train_data['label'])\n",
    "\n",
    "dev_data['label'] = dev_data['label'].map(label_map)\n",
    "print(dev_data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82bab64a-e3e9-4737-88f8-3069d51eea71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_data = train_data.drop('solved_conflict', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b0e43fd-b039-4889-a78e-95c25006ceb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3973</th>\n",
       "      <td>c3b6ccdb-a1c3-4e94-b8b1-a92c6c2b2987</td>\n",
       "      <td>spiega la prof di matematica</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2457</th>\n",
       "      <td>2283e7e30f82a6ee90882c5a6f057b0a4bd6d0b8</td>\n",
       "      <td>Die syrischen Soldaten haben bei ihrem Vormars...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1422</th>\n",
       "      <td>5eb8425e-45f5-4e9e-a360-aaf086b58fa0</td>\n",
       "      <td>Is monkeypox going to be the “cause” of anothe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1601</th>\n",
       "      <td>9fe9e827-93b0-4222-8f42-91b16b5cc715</td>\n",
       "      <td>The Feds staked out various Feeding Our Future...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1817</th>\n",
       "      <td>b0391982-3045-4447-96d3-e60024cdd491</td>\n",
       "      <td>The Left aims to make the hordes of illegal al...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3766</th>\n",
       "      <td>27167802-98f4-41b3-b5c5-433165b48881</td>\n",
       "      <td>Una manifestazione finita tragicamente, con la...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2828</th>\n",
       "      <td>b1f15f15-cbbd-4675-a42a-d9172f5f9529</td>\n",
       "      <td>E sull’attacco dello scorso agosto:</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2638</th>\n",
       "      <td>6dae6a67091b03b7d9d7c51f95fe33d3a02c7377</td>\n",
       "      <td>Allesamt unannehmbare Bedingungen für „Grüne“ ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3974</th>\n",
       "      <td>4bef47c4-0534-4893-b0bd-30eb3fe6c11e</td>\n",
       "      <td>Chissà cosa ne ha pensato Stephanie Williams, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>MIS_973-curl_02_016</td>\n",
       "      <td>وقال ترامب إن منصات التواصل الاجتماعى بات لديه...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1500 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   sentence_id  \\\n",
       "3973      c3b6ccdb-a1c3-4e94-b8b1-a92c6c2b2987   \n",
       "2457  2283e7e30f82a6ee90882c5a6f057b0a4bd6d0b8   \n",
       "1422      5eb8425e-45f5-4e9e-a360-aaf086b58fa0   \n",
       "1601      9fe9e827-93b0-4222-8f42-91b16b5cc715   \n",
       "1817      b0391982-3045-4447-96d3-e60024cdd491   \n",
       "...                                        ...   \n",
       "3766      27167802-98f4-41b3-b5c5-433165b48881   \n",
       "2828      b1f15f15-cbbd-4675-a42a-d9172f5f9529   \n",
       "2638  6dae6a67091b03b7d9d7c51f95fe33d3a02c7377   \n",
       "3974      4bef47c4-0534-4893-b0bd-30eb3fe6c11e   \n",
       "237                        MIS_973-curl_02_016   \n",
       "\n",
       "                                               sentence  label  \n",
       "3973                       spiega la prof di matematica      0  \n",
       "2457  Die syrischen Soldaten haben bei ihrem Vormars...      0  \n",
       "1422  Is monkeypox going to be the “cause” of anothe...      1  \n",
       "1601  The Feds staked out various Feeding Our Future...      0  \n",
       "1817  The Left aims to make the hordes of illegal al...      1  \n",
       "...                                                 ...    ...  \n",
       "3766  Una manifestazione finita tragicamente, con la...      1  \n",
       "2828                E sull’attacco dello scorso agosto:      0  \n",
       "2638  Allesamt unannehmbare Bedingungen für „Grüne“ ...      1  \n",
       "3974  Chissà cosa ne ha pensato Stephanie Williams, ...      1  \n",
       "237   وقال ترامب إن منصات التواصل الاجتماعى بات لديه...      0  \n",
       "\n",
       "[1500 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb5e64bf-6878-4798-909e-8cafd20d509d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "from transformers import XLMRobertaTokenizer, XLMRobertaForSequenceClassification, Trainer, TrainingArguments\n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9316a84c-fbfa-4470-a6d6-7bd982b1d878",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings = tokenizer(train_data['sentence'].tolist(), truncation=True, padding=True)\n",
    "eval_encodings = tokenizer(dev_data['sentence'].tolist(), truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8410770-d13a-4171-a1ab-8649cea1a448",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_encodings[\"input_ids\"]\n",
    "#train_encodings[\"attention_mask\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54d733c6-fe92-4669-be32-2d41330930c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PyTorch datasets\n",
    "train_dataset = torch.utils.data.TensorDataset(\n",
    "    torch.tensor(train_encodings['input_ids']),\n",
    "    torch.tensor(train_encodings['attention_mask']),\n",
    "    torch.tensor(train_data['label'].to_numpy())\n",
    ")\n",
    "\n",
    "val_dataset = torch.utils.data.TensorDataset(\n",
    "    torch.tensor(eval_encodings['input_ids']),\n",
    "    torch.tensor(eval_encodings['attention_mask']),\n",
    "    torch.tensor(dev_data['label'].to_numpy())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fef0ac7b-eddf-4d72-a6e5-ed40fd5c0704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([189])\n",
      "torch.Size([189])\n",
      "torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[0][0].shape) # input_ids shape\n",
    "print(train_dataset[0][1].shape) # attention_mask shape\n",
    "print(train_dataset[0][2].shape) # train_labels_onehot shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e52d0654-f37c-45a9-8cf0-f0eb9394f709",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Define model\n",
    "model = XLMRobertaForSequenceClassification.from_pretrained('xlm-roberta-large', num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc35ab52-6ca6-4384-a211-736850a725fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "device\n",
    "\n",
    "#device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c5047e6e-4428-4172-bcf1-582d78d540c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer parameters\n",
    "epochs = 3\n",
    "learning_rate=5e-5 # default 5e-5\n",
    "warmup_steps = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "561d3b8a-6e2e-4e74-8d5b-365031776a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=epochs,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    warmup_steps=warmup_steps,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy='steps',\n",
    "    eval_steps=50,\n",
    "    learning_rate=learning_rate,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "493e86e5-a63f-40dc-b6bb-09e3bf48a3c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kuba\\Documents\\py_interpreter2\\Lib\\site-packages\\accelerate\\accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Define the trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=lambda data: {'input_ids': torch.stack([item[0] for item in data]),\n",
    "                                'attention_mask': torch.stack([item[1] for item in data]),\n",
    "                                'labels': torch.stack([item[2] for item in data])},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "96eb11d6-0c5c-4d85-b10d-0e2a028b0f8e",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# epoch 4 / 50 = 200\t0.538800\t0.644553\u001b[39;00m\n",
      "File \u001b[1;32m~\\Documents\\py_interpreter2\\Lib\\site-packages\\transformers\\trainer.py:1780\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1778\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1779\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1780\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1781\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1782\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1783\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1784\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1785\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\py_interpreter2\\Lib\\site-packages\\transformers\\trainer.py:2118\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2115\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m   2117\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m-> 2118\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2121\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2122\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2123\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2124\u001b[0m ):\n\u001b[0;32m   2125\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2126\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32m~\\Documents\\py_interpreter2\\Lib\\site-packages\\transformers\\trainer.py:3045\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   3043\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m   3044\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3045\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3047\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n",
      "File \u001b[1;32m~\\Documents\\py_interpreter2\\Lib\\site-packages\\accelerate\\accelerator.py:2013\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[1;34m(self, loss, **kwargs)\u001b[0m\n\u001b[0;32m   2011\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   2012\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2013\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\py_interpreter2\\Lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\py_interpreter2\\Lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# epoch 4 / 50 = 200\t0.538800\t0.644553"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94836ea-f1b4-4a3c-9a28-ff59e88400fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a43a6c-c16c-4727-a3e0-535b08399f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save_pretrained(\"./model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be77525e-b65e-4001-9cf2-e944bf62f142",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = XLMRobertaForSequenceClassification.from_pretrained(\"./model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed6e01f-b6c1-4fbc-b6eb-08fb20e1984f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "test_data = pd.read_csv(\"data/subtask-2-multilingual/ml_only_2024_languages/dev_test_ml_only_2024_languages.tsv\", sep='\\t')  # Update with your dev data file\n",
    "test_data['label'] = test_data['label'].map(label_map)\n",
    "test_encodings = tokenizer(test_data['sentence'].tolist(), truncation=True, padding=True)\n",
    "\n",
    "test_dataset = torch.utils.data.TensorDataset(\n",
    "    torch.tensor(test_encodings['input_ids']),\n",
    "    torch.tensor(test_encodings['attention_mask']),\n",
    "    torch.tensor(test_data['label'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27e6a25-370d-46ca-a31a-7ecaa349c6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = trainer.predict(test_dataset)\n",
    "pred_labels = preds.predictions.argmax(-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43978ae1-955e-4775-8566-3b654114c453",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150f12f8-f0cd-4a7d-aa88-d18edb5c33f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the true labels to integers\n",
    "\n",
    "# Compute the accuracy and classification report\n",
    "accuracy = accuracy_score(test_data['label'], pred_labels)\n",
    "class_report = classification_report(test_data['label'], pred_labels, target_names=['OBJ', 'SUBJ'])\n",
    "\n",
    "print(f\"Accuracy for RoBERTa: {accuracy}\")\n",
    "print(f\"Classification Report:\\n{class_report}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67dacc59-1c15-4158-9d99-edd69d90bd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate 5e-5\n",
    "# epochs 3 - acc 0.61 - OBJ 0.56, SUBJ 0.71 - f1 0.66 - 0.54"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
